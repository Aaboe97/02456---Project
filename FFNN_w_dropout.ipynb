{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0441382d-435c-4bda-a1fd-3e246297a80c",
   "metadata": {},
   "source": [
    "# Working w. MNIST and weight drop out\n",
    "\n",
    "\n",
    "In this we turn of a random section of the nodes in a hidden layer given a drop out fraction p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b478b77-3090-4e23-957c-0aa46604a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow\n",
    "\n",
    "def softmax(y_hat):\n",
    "    '''\n",
    "    INPUT \n",
    "    y_hat: The output of the feedforward pure procedure. \n",
    "\n",
    "    OUTPUT\n",
    "    The softmax of y_hat\n",
    "    '''\n",
    "    y_hat = y_hat - np.max(y_hat, axis=0, keepdims=True)  # prevent overflow\n",
    "    exp_scores = np.exp(y_hat)\n",
    "    return exp_scores / np.sum(exp_scores, axis=0, keepdims=True)\n",
    "\n",
    "# Initialize Weights\n",
    "\n",
    "def init(dims):\n",
    "    \"\"\"\n",
    "    Initialize weights for a multi-layer feedforward neural network.\n",
    "    \n",
    "    INPUT:\n",
    "        dims : Layer dimensions [input, hidden1, hidden2, ..., output]\n",
    "    \n",
    "    OUTPUT:\n",
    "        W : List of weight matrices for each layer.\n",
    "            Each W[l] has shape (dims[l]+1, dims[l+1]) and includes bias weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    W = []\n",
    "    for i in range(len(dims)-1):\n",
    "        W.append(np.random.randn(dims[i]+1, dims[i+1]) * np.sqrt(2 / dims[i]))\n",
    "    return W\n",
    "\n",
    "# Forward Pass\n",
    "\n",
    "def forward(X, W, dropout_on=False, dropout_p=None):\n",
    "    \"\"\"\n",
    "    Perform the forward pass through all layers, with dropout possible. The dropout is done on each hidden unit layer, \n",
    "    where a different dropout % is possible for each layer, here we turn off hidden units aka set them to 0, so its as close to the dropout %. \n",
    "    \n",
    "    The still turned on hidden units are scaled so the total \"energy/value\" of the rest of the hidden units are the same as before turn off. \n",
    "    \n",
    "    INPUT:\n",
    "        X: Input data (shape: features × # of batches)\n",
    "        W: Weight matrices for each layer\n",
    "        dropout_on: True/False statement about the use of dropout\n",
    "        dropout_p: dropout percentage for each hidden layer, not used in the input or output layer. \n",
    "    \n",
    "    OUTPUT:\n",
    "        y : Output of forward with softmax applied\n",
    "        h : List containing activations (hidden layers and final output pre-softmax)\n",
    "        masks: contain the dropout masks used, so that they can be used in the backwards propagation. \n",
    "    \"\"\"\n",
    "    #initalize \n",
    "    h = []\n",
    "    masks = []\n",
    "    a = X.copy()\n",
    "    num_hidden = len(W)-1\n",
    "    \n",
    "    # define p for when no dropout matrix is given, and check if the p vector is the right size. \n",
    "    if dropout_p is None:\n",
    "        dropout_p = [0.0]*num_hidden\n",
    "    elif len(dropout_p) != num_hidden:\n",
    "        raise ValueError(f\"dropout_p must have {num_hidden} values\")\n",
    "\n",
    "    # Loop through hidden layers\n",
    "    for l in range(num_hidden):\n",
    "        #input layer\n",
    "        a = np.vstack([a, np.ones(a.shape[1])])  # add bias\n",
    "        z = W[l].T @ a\n",
    "        a = np.maximum(0, z)  # ReLU\n",
    "\n",
    "        # hidden layer\n",
    "        if dropout_on and dropout_p[l] > 0.0:\n",
    "            p = dropout_p[l]\n",
    "            mask = (np.random.rand(*a.shape) > p).astype(float) / (1.0 - p) # random numbers 0-1 with length a, scaled so activation energy preserved. \n",
    "            a *= mask\n",
    "            \n",
    "            # Debug: check dropout\n",
    "            #frac_active_per_neuron = np.sum(mask != 0, axis=1) / mask.shape[1] # for each sample fraction turned off\n",
    "            #print(f\"Layer {l}: avg {100*(1-np.mean(frac_active_per_neuron)):.1f}% neurons turned off\") # average for batch \n",
    "\n",
    "        else:\n",
    "            mask = np.ones_like(a) # set all to 1 if there is not dropout \n",
    "\n",
    "        h.append(a)\n",
    "        masks.append(mask)\n",
    "\n",
    "    # Output layer\n",
    "    a = np.vstack([a, np.ones(a.shape[1])])\n",
    "    y_hat = W[-1].T @ a\n",
    "    y = softmax(y_hat)\n",
    "    return y, h, masks\n",
    "\n",
    "\n",
    "# Backward Pass\n",
    "\n",
    "def backward(X, T, W, y, h, masks, eta):\n",
    "    \"\"\"\n",
    "    Perform one backward pass and update weights.\n",
    "    \n",
    "    INPUT:\n",
    "        X : Input data (features × samples)\n",
    "        T : Target labels (one-hot encoded)\n",
    "        W : Current weight matrices\n",
    "        eta : Learning rate\n",
    "        masks: The dropout mask\n",
    "    \n",
    "    OUTPUT:\n",
    "        W : Updated weight matrices\n",
    "        loss : Total loss for this batch\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    delta = y - T\n",
    "    num_hidden = len(W) - 1\n",
    "\n",
    "    # Output layer\n",
    "    \n",
    "    a_prev = np.vstack([h[-1], np.ones(h[-1].shape[1])])  # last hidden -> output\n",
    "    Q = a_prev @ delta.T\n",
    "    W[-1] -= (eta / m) * Q\n",
    "    delta = W[-1][:-1, :] @ delta  # backprop to last hidden layer\n",
    "\n",
    "    # loop hidden layers \n",
    "\n",
    "    for l in range(num_hidden-1, 0, -1):  \n",
    "        relu_grad = (h[l] > 0).astype(float)\n",
    "        delta *= relu_grad * masks[l]  # apply mask\n",
    "\n",
    "        a_prev = np.vstack([h[l-1], np.ones(h[l-1].shape[1])])\n",
    "        Q = a_prev @ delta.T\n",
    "        W[l] -= (eta / m) * Q\n",
    "\n",
    "        delta = W[l][:-1, :] @ delta  # propagate delta backward\n",
    "\n",
    "    # First hidden layer\n",
    "    \n",
    "    relu_grad = (h[0] > 0).astype(float)\n",
    "    delta *= relu_grad * masks[0]\n",
    "\n",
    "    a_prev = np.vstack([X, np.ones(X.shape[1])])  # input -> first hidden\n",
    "    Q = a_prev @ delta.T\n",
    "    W[0] -= (eta / m) * Q\n",
    "\n",
    "    # Compute loss\n",
    "    epsilon = 1e-12\n",
    "    loss = -np.sum(np.log(np.sum(y * T, axis=0) + epsilon))\n",
    "    return W, loss\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "def train(X, T, W, epochs, eta, batchsize=32, dropout_on=False, dropout_p=None):\n",
    "    \"\"\"\n",
    "    Train the multi-layer neural network using gradient descent.\n",
    "    \n",
    "    INPUT:\n",
    "        X : Input data (features × samples)\n",
    "        T : Target one-hot labels (classes × samples)\n",
    "        W : Initialized weight matrices\n",
    "        epochs :  Number of training epochs\n",
    "        eta : Learning rate\n",
    "        batchsize : Size of each training batch\n",
    "        dropout_on: True/False statement about the use of dropout\n",
    "        dropout_p: dropout percentage for each hidden layer, not used in the input or output layer.\n",
    "    \n",
    "    OUTPUT:\n",
    "        W : Trained weight matrices\n",
    "        losses : Total loss for each epoch\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        order = np.random.permutation(m)\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, m, batchsize):\n",
    "            batch = order[i:i+batchsize]\n",
    "            X_batch = X[:, batch]\n",
    "            T_batch = T[:, batch]\n",
    "\n",
    "            y, h, masks = forward(X_batch, W, dropout_on, dropout_p)\n",
    "            W, loss = backward(X_batch, T_batch, W, y, h, masks, eta)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "    return W, losses\n",
    "\n",
    "\n",
    "# Prediction\n",
    "\n",
    "def predict(X, W):\n",
    "    '''\n",
    "    INPUT:\n",
    "    X: Test data \n",
    "    W: Weight matrices\n",
    "\n",
    "    OUTPUT:\n",
    "    The predicted label. \n",
    "\n",
    "    '''\n",
    "    y, _, _ = forward(X, W, dropout_on=False)\n",
    "    return np.argmax(y, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee12048a-d984-479d-875e-32f7068b206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 128821.7245\n",
      "Epoch 2/50, Loss: 108043.8058\n",
      "Epoch 3/50, Loss: 94840.6707\n",
      "Epoch 4/50, Loss: 85533.9991\n",
      "Epoch 5/50, Loss: 78925.6997\n",
      "Epoch 6/50, Loss: 73872.2594\n",
      "Epoch 7/50, Loss: 69590.5968\n",
      "Epoch 8/50, Loss: 66333.7258\n",
      "Epoch 9/50, Loss: 63592.8952\n",
      "Epoch 10/50, Loss: 60724.1267\n",
      "Epoch 11/50, Loss: 58777.9915\n",
      "Epoch 12/50, Loss: 57161.3534\n",
      "Epoch 13/50, Loss: 55395.6671\n",
      "Epoch 14/50, Loss: 53796.4436\n",
      "Epoch 15/50, Loss: 52827.3757\n",
      "Epoch 16/50, Loss: 51152.9119\n",
      "Epoch 17/50, Loss: 50386.1601\n",
      "Epoch 18/50, Loss: 49093.9597\n",
      "Epoch 19/50, Loss: 48389.3018\n",
      "Epoch 20/50, Loss: 47471.6146\n",
      "Epoch 21/50, Loss: 46748.7468\n",
      "Epoch 22/50, Loss: 45835.6456\n",
      "Epoch 23/50, Loss: 45455.7021\n",
      "Epoch 24/50, Loss: 44427.3001\n",
      "Epoch 25/50, Loss: 43700.3616\n",
      "Epoch 26/50, Loss: 43451.5236\n",
      "Epoch 27/50, Loss: 42710.6251\n",
      "Epoch 28/50, Loss: 42418.4476\n",
      "Epoch 29/50, Loss: 41850.4140\n",
      "Epoch 30/50, Loss: 41579.9785\n",
      "Epoch 31/50, Loss: 41315.8041\n",
      "Epoch 32/50, Loss: 40723.1266\n",
      "Epoch 33/50, Loss: 39808.1450\n",
      "Epoch 34/50, Loss: 40022.9174\n",
      "Epoch 35/50, Loss: 39224.7602\n",
      "Epoch 36/50, Loss: 39271.4545\n",
      "Epoch 37/50, Loss: 38721.4605\n",
      "Epoch 38/50, Loss: 38684.5662\n",
      "Epoch 39/50, Loss: 38095.8767\n",
      "Epoch 40/50, Loss: 37903.7245\n",
      "Epoch 41/50, Loss: 37406.6376\n",
      "Epoch 42/50, Loss: 37324.6741\n",
      "Epoch 43/50, Loss: 36723.3501\n",
      "Epoch 44/50, Loss: 36971.8099\n",
      "Epoch 45/50, Loss: 36079.6834\n",
      "Epoch 46/50, Loss: 36553.9999\n",
      "Epoch 47/50, Loss: 36123.0087\n",
      "Epoch 48/50, Loss: 35984.1969\n",
      "Epoch 49/50, Loss: 35372.3358\n",
      "Epoch 50/50, Loss: 35300.6289\n",
      "Test Accuracy: 92.58%\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Load MNIST\n",
    "# --------------------------\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 28*28) / 255.0\n",
    "X_test  = X_test.reshape(-1, 28*28) / 255.0\n",
    "T_train = to_categorical(y_train, num_classes=10)\n",
    "T_test  = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# --------------------------\n",
    "# Initialize network\n",
    "# --------------------------\n",
    "\n",
    "dims = [784, 32, 32, 10]\n",
    "W = init(dims)\n",
    "\n",
    "# --------------------------\n",
    "# Train\n",
    "# --------------------------\n",
    "\n",
    "# dropout fractions: hidden layers\n",
    "dropout_p = [0.3, 0.5]  # hidden1 30%, hidden2 50%\n",
    "\n",
    "W, losses = train(\n",
    "    X_train.T, T_train.T, W,\n",
    "    epochs=50,\n",
    "    eta=0.001,\n",
    "    batchsize=32,\n",
    "    dropout_on=True,\n",
    "    dropout_p=dropout_p\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Evaluate\n",
    "# --------------------------\n",
    "\n",
    "y_pred = predict(X_test.T, W)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
