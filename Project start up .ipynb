{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a618c9",
   "metadata": {},
   "source": [
    "# FFNN: Single layer example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def init(dims):\n",
    "    # We initalize the weights in this case I use where there are a input layer x layer z and output y,\n",
    "    # so z+y weights to initalize. The initalized are scale for input, so its variance scaled correctly.\n",
    "    \n",
    "    # INPUT: Dimesnion of the single layer network\n",
    "    # OUTPUT: Initalized weight matrix, with each column is a weight fx x->z.\n",
    "    \n",
    "    x,z,y=dims\n",
    "    w = np.sqrt(1/(x+1))\n",
    "    W = []\n",
    "    W.append(w*np.random.randn(x+1,z))\n",
    "    w = np.sqrt(1/(z+1))\n",
    "    W.append(w*np.random.randn(z+1,y))\n",
    "    return W\n",
    "\n",
    "def forward(X, W, n):\n",
    "    # INPUT: X: Training data, W: Weight matrix, n: points used for training. \n",
    "    # OUTPUT: y: Outbut of FFNN, h: a, activation for hidden layer.  \n",
    "    \n",
    "    # A fully connected neural network with a_1=max(0,z) (ReLU) as acitvation func and y_out=a_out=softmax(y)\n",
    "    \n",
    "    # Look only at subsection of data ( a bit too simple, but for example sake)\n",
    "    x = X[:,0:n]\n",
    "    x.shape\n",
    "    x = np.vstack((x, np.ones((1,n))))\n",
    "    x.shape\n",
    "\n",
    "    # % FFNN\n",
    "    # First layer\n",
    "    z = W[0].T@x\n",
    "    h = np.maximum(0, z)\n",
    "    \n",
    "    # Second layer\n",
    "    y_hat = W[1].T@np.vstack((h, np.ones((1,n))))\n",
    "    y = np.exp(y_hat)/(np.exp(y_hat).sum(axis=0))\n",
    "    \n",
    "    return y, h\n",
    "\n",
    "def backward(X, T, W, n,lr):\n",
    "    # INPUT: X: Data, T: training sta, W: weights, n: nr trainings points, lr: Learning rate\n",
    "    \n",
    "    # Perfom foreward \n",
    "    y, h = forward(X, W, n)\n",
    "    \n",
    "    \n",
    "    x = X[:,0:n]\n",
    "    x.shape\n",
    "    x = np.vstack((x, np.ones((1,n))))\n",
    "    x.shape\n",
    "\n",
    "    delta2=y-T[:,0:n] # Very simple error function of error = model - true = delta2(y)\n",
    "    Q2=np.vstack((h, np.ones((1,n))))@delta2.T  # dE/d W^2 = h * delta_2 \n",
    "    W_hat=W[1]                                  # Weight from input layer W^1\n",
    "    W_hat=W_hat[:-1,:]                          # Remove bias weight from W^1\n",
    "    a_m=np.where(h > 0, 1, 0)                   # Defin the derivativ of ReLU a'=ReLu'\n",
    "    delta1=a_m*(W_hat@delta2)                   # delta_1=a'*(W^1*delta_2)\n",
    "    Q1=x@delta1.T                               # dE/dW^1 = x * delta_1\n",
    "    \n",
    "    #Gradient Decent \n",
    "    W[0]=W[0]-lr*Q1\n",
    "    W[1]=W[1]-lr*Q2\n",
    "    print(f'loop {i}')\n",
    "    return W\n",
    "\n",
    "def train(X, T, dims, epochs, lr, batchsize=1):\n",
    "\n",
    "    # Train the 2-layer network using mini-batch gradient descent.\n",
    "    \n",
    "    # INPUT: X : Training input data, T : Target, dims : (x, z, y) layer size, epochs : Number of epochs\n",
    "    #        lr : Learning rate, batchsize :  Size of mini-batch\n",
    "            \n",
    "    # OUTPUT: W : Trained weights,  losses :Loss at each epoch\n",
    " \n",
    "    \n",
    "    rg = np.random.default_rng()\n",
    "    W = init(dims)\n",
    "    losses = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        order = rg.permutation(m)\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for k in range(0, m, batchsize):\n",
    "            batch = order[k:k + batchsize]\n",
    "            X_batch = X[:, batch]\n",
    "            T_batch = T[:, batch]\n",
    "            W, loss = backward(X_batch, T_batch, W, n=batchsize, lr=lr)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Epoch {e+1}, loss {epoch_loss:.4f}\")\n",
    "    \n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441220a",
   "metadata": {},
   "source": [
    "# FFNN: Multilayer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40474874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rg = np.random.default_rng() # random munber generator \n",
    "\n",
    "def init(dims):\n",
    "    \"\"\"\n",
    "    Initialize weights for a multi-layer feedforward neural network.\n",
    "    \n",
    "    INPUT:\n",
    "        dims : Layer dimensions [input, hidden1, hidden2, ..., output]\n",
    "    \n",
    "    OUTPUT:\n",
    "        W : List of weight matrices for each layer.\n",
    "            Each W[l] has shape (dims[l]+1, dims[l+1]) and includes bias weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    W = []\n",
    "    for l in range(len(dims) - 1):\n",
    "        \n",
    "        # initialization for ReLU layers: variance = 2 / fan_in, makes sure variance is stable for relu \n",
    "        fan_in = dims[l] + 1\n",
    "        scale = np.sqrt(2 / fan_in)\n",
    "        W.append(scale * rg.standard_normal(size=(fan_in, dims[l+1])))\n",
    "        \n",
    "    return W\n",
    "\n",
    "\n",
    "def forward(X, W):\n",
    "    \"\"\"\n",
    "    Perform the forward pass through all layers.\n",
    "    \n",
    "    INPUT:\n",
    "        X : Input data (shape: features × samples)\n",
    "        W : Weight matrices for each layer\n",
    "    \n",
    "    OUTPUT:\n",
    "        y : Output of forward with softmax aplied\n",
    "        h : List containing activations (hidden layers and final output pre-softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    h = []  # store activations of each layer\n",
    "    a = X   # input layer\n",
    "    \n",
    "    # Forward through hidden layers\n",
    "    for l in range(len(W) - 1):\n",
    "        # Add bias term\n",
    "        a = np.vstack((a, np.ones((1, a.shape[1]))))\n",
    "        \n",
    "        # Linear transform\n",
    "        z = W[l].T @ a\n",
    "        \n",
    "        # ReLU activation\n",
    "        a = np.maximum(0, z)\n",
    "        h.append(a)\n",
    "    \n",
    "    # Output layer (no ReLU, only linear before softmax)\n",
    "    a = np.vstack((a, np.ones((1, a.shape[1]))))\n",
    "    y_hat = W[-1].T @ a\n",
    "    h.append(y_hat)\n",
    "    \n",
    "    # Last element in h is pre-softmax output\n",
    "    y_hat = h[-1]\n",
    "    \n",
    "    # Numerically stable softmax\n",
    "    exp_y = np.exp(y_hat - np.max(y_hat, axis=0))\n",
    "    y = exp_y / exp_y.sum(axis=0)\n",
    "    \n",
    "    return y, h\n",
    "\n",
    "\n",
    "def backward(X, T, W, lr):\n",
    "    \"\"\"\n",
    "    Perform one backward pass and update weights.\n",
    "    \n",
    "    INPUT:\n",
    "        X : Input data (features × samples)\n",
    "        T : Target labels (one-hot encoded)\n",
    "        W : Current weight matrices\n",
    "        lr : Learning rate\n",
    "    \n",
    "    OUTPUT:\n",
    "        W : Updated weight matrices\n",
    "        loss : Total loss for this batch\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X.shape[1]\n",
    "    y, h = forward(X, W)\n",
    "\n",
    "    # --- Loss ---\n",
    "    loss = -np.sum(T * np.log(y + 1e-15)) # cross-entropy loss for multi-class classification\n",
    "    \n",
    "    # --- Backpropagation --- (Like in example above just more layers)\n",
    "    delta = y - T  # output layer error (simple example)\n",
    "    \n",
    "    # --- First layer update ---\n",
    "    x_bias = np.vstack((X, np.ones((1, n))))\n",
    "    Q0 = x_bias @ delta.T\n",
    "    W[0] -= (lr / n) * Q0\n",
    "    \n",
    "    # --- Update all other layers\n",
    "    \n",
    "    for l in range(len(W) - 1, 0, -1):\n",
    "        # Get hidden activation for this layer\n",
    "        h_l = h[l-1] if l-1 >= 0 else X\n",
    "        \n",
    "        # Add bias term\n",
    "        h_l = np.vstack((h_l, np.ones((1, h_l.shape[1]))))\n",
    "        \n",
    "        # Compute gradient for this layer\n",
    "        Q = h_l @ delta.T\n",
    "        \n",
    "        # Update weights\n",
    "        W[l] -= (lr / n) * Q\n",
    "        \n",
    "        # Backpropagate error to previous layer\n",
    "        W_hat = W[l][:-1, :]  # remove bias weights\n",
    "        \n",
    "        if l > 0: # compute delta for next layer\n",
    "            a_prime = (h[l-1] > 0).astype(float)  # ReLU derivative\n",
    "            delta = a_prime * (W_hat @ delta)\n",
    "    \n",
    "    return W, loss\n",
    "\n",
    "\n",
    "def train(X, T, W, epochs, lr, batchsize=1):\n",
    "    \"\"\"\n",
    "    Train the multi-layer neural network using gradient descent.\n",
    "    \n",
    "    INPUT:\n",
    "        X : Input data (features × samples)\n",
    "        T : Target one-hot labels (classes × samples)\n",
    "        W : Initialized weight matrices\n",
    "        epochs :  Number of training epochs\n",
    "        lr : Learning rate\n",
    "        batchsize : Size of each training batch\n",
    "    \n",
    "    OUTPUT:\n",
    "        W : Trained weight matrices\n",
    "        losses : Total loss for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    losses = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        order = rg.permutation(m) # mini-batch is drawn in a random order\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for k in range(0, m, batchsize):\n",
    "            batch = order[k:k+batchsize]\n",
    "            X_batch = X[:, batch]\n",
    "            T_batch = T[:, batch]\n",
    "            \n",
    "            W, loss = backward(X_batch, T_batch, W, lr)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Epoch {e+1}, loss {epoch_loss:.4f}\")\n",
    "    \n",
    "    return W, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23528dcc",
   "metadata": {},
   "source": [
    "# Working w. MNIST minimal change \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4eb2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow\n",
    "\n",
    "def softmax(y_hat):\n",
    "    y_hat = y_hat - np.max(y_hat, axis=0, keepdims=True)  # prevent overflow\n",
    "    exp_scores = np.exp(y_hat)\n",
    "    return exp_scores / np.sum(exp_scores, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "### 2. Load MNIST Data\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape and normalize inputs \n",
    "X_train = X_train.reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.reshape(-1, 28*28) / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "T_train = to_categorical(y_train, num_classes=10)\n",
    "T_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "\n",
    "### 3. Initialize Network Parameters\n",
    "\n",
    "def init(dims):\n",
    "    W = []\n",
    "    for i in range(len(dims) - 1):\n",
    "        W.append(np.random.randn(dims[i] + 1, dims[i + 1]) * np.sqrt(2 / (dims[i])))\n",
    "    return W\n",
    "\n",
    "dims = [784, 32, 32, 10]  # Input layer (784), hidden layer (128), output layer (10)\n",
    "W = init(dims)\n",
    "\n",
    "\n",
    "### 4. Define Forward Pass\n",
    "\n",
    "def forward(X, W):\n",
    "    h = []\n",
    "    a = X\n",
    "    for l in range(len(W) - 1):\n",
    "        a = np.vstack([a, np.ones(a.shape[1])])  # Add bias term\n",
    "        z = W[l].T @ a\n",
    "        a = np.maximum(0, z)  # ReLU activation\n",
    "        h.append(a)\n",
    "    a = np.vstack([a, np.ones(a.shape[1])])  # Add bias term\n",
    "    y_hat = W[-1].T @ a\n",
    "    y = softmax(y_hat)  # new stable version\n",
    "  # Softmax\n",
    "    return y, h\n",
    "\n",
    "\n",
    "### 5. Define Backward Pass\n",
    "\n",
    "def backward(X, T, W, h, eta):\n",
    "    m = X.shape[1]\n",
    "    y, _ = forward(X, W)\n",
    "    delta = y - T\n",
    "    for l in range(len(W) - 1, 0, -1):\n",
    "        a_prev = np.vstack([h[l-1], np.ones(h[l-1].shape[1])])  # Add bias term\n",
    "        Q = a_prev @ delta.T\n",
    "        W[l] -= (eta / m) * Q\n",
    "        delta = W[l][:-1, :] @ delta\n",
    "        delta *= h[l-1] > 0  # ReLU derivative\n",
    "    a_prev = np.vstack([X, np.ones(X.shape[1])])  # Add bias term\n",
    "    Q = a_prev @ delta.T\n",
    "    W[0] -= eta * Q\n",
    "    epsilon = 1e-12\n",
    "    loss = -np.sum(np.log(np.sum(y * T, axis=0) + epsilon))\n",
    "    return W, loss\n",
    "\n",
    "\n",
    "### 6. Training Loop\n",
    "\n",
    "def train(X, T, W, epochs, eta, batchsize=32):\n",
    "    m = X.shape[1]\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        order = np.random.permutation(m)\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, m, batchsize):\n",
    "            batch = order[i:i+batchsize]\n",
    "            X_batch = X[:, batch]\n",
    "            T_batch = T[:, batch]\n",
    "            _, h = forward(X_batch, W)\n",
    "            W, loss = backward(X_batch, T_batch, W, h, eta)\n",
    "            epoch_loss += loss\n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    return W, losses\n",
    "\n",
    "\n",
    "### 7. Train the Model\n",
    "\n",
    "epochs = 100\n",
    "eta = 0.001\n",
    "W, losses = train(X_train.T, T_train.T, W, epochs, eta)\n",
    "\n",
    "### 8. Evaluate the Model\n",
    "\n",
    "def predict(X, W):\n",
    "    y, _ = forward(X, W)\n",
    "    return np.argmax(y, axis=0)\n",
    "\n",
    "y_pred = predict(X_test.T, W)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
