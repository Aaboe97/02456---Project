{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a618c9",
   "metadata": {},
   "source": [
    "# FFNN: Single layer example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab93e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(dims):\n",
    "    # We initalize the weights in this case I use where there are a input layer x layer z and output y,\n",
    "    # so z+y weights to initalize. The initalized are scale for input, so its variance scaled correctly.\n",
    "    \n",
    "    # INPUT: Dimesnion of the single layer network\n",
    "    # OUTPUT: Initalized weight matrix, with each column is a weight fx x->z.\n",
    "    \n",
    "    x,z,y=dims\n",
    "    w = np.sqrt(1/(x+1))\n",
    "    W = []\n",
    "    W.append(w*np.random.randn(x+1,z))\n",
    "    w = np.sqrt(1/(z+1))\n",
    "    W.append(w*np.random.randn(z+1,y))\n",
    "    return W\n",
    "\n",
    "def forward(X, W, n):\n",
    "    # INPUT: X: Training data, W: Weight matrix, n: points used for training. \n",
    "    # OUTPUT: y: Outbut of FFNN, h: a, activation for hidden layer.  \n",
    "    \n",
    "    # A fully connected neural network with a_1=max(0,z) (ReLU) as acitvation func and y_out=a_out=softmax(y)\n",
    "    \n",
    "    # Look only at subsection of data ( a bit too simple, but for example sake)\n",
    "    x = X[:,0:n]\n",
    "    x.shape\n",
    "    x = np.vstack((x, np.ones((1,n))))\n",
    "    x.shape\n",
    "\n",
    "    # % FFNN\n",
    "    # First layer\n",
    "    z = W[0].T@x\n",
    "    h = np.maximum(0, z)\n",
    "    \n",
    "    # Second layer\n",
    "    y_hat = W[1].T@np.vstack((h, np.ones((1,n))))\n",
    "    y = np.exp(y_hat)/(np.exp(y_hat).sum(axis=0))\n",
    "    \n",
    "    return y, h\n",
    "\n",
    "def backward(X, T, W, n,lr):\n",
    "    # INPUT: X: Data, T: training sta, W: weights, n: nr trainings points, lr: Learning rate\n",
    "    \n",
    "    # Perfom foreward \n",
    "    y, h = forward(X, W, n)\n",
    "    \n",
    "    \n",
    "    x = X[:,0:n]\n",
    "    x.shape\n",
    "    x = np.vstack((x, np.ones((1,n))))\n",
    "    x.shape\n",
    "\n",
    "    delta2=y-T[:,0:n] # Very simple error function of error = model - true = delta2(y)\n",
    "    Q2=np.vstack((h, np.ones((1,n))))@delta2.T  # dE/d W^2 = h * delta_2 \n",
    "    W_hat=W[1]                                  # Weight from input layer W^1\n",
    "    W_hat=W_hat[:-1,:]                          # Remove bias weight from W^1\n",
    "    a_m=np.where(h > 0, 1, 0)                   # Defin the derivativ of ReLU a'=ReLu'\n",
    "    delta1=a_m*(W_hat@delta2)                   # delta_1=a'*(W^1*delta_2)\n",
    "    Q1=x@delta1.T                               # dE/dW^1 = x * delta_1\n",
    "    \n",
    "    #Gradient Decent \n",
    "    W[0]=W[0]-lr*Q1\n",
    "    W[1]=W[1]-lr*Q2\n",
    "    print(f'loop {i}')\n",
    "    return W\n",
    "\n",
    "def train(X, T, dims, epochs, lr, batchsize=1):\n",
    "\n",
    "    # Train the 2-layer network using mini-batch gradient descent.\n",
    "    \n",
    "    # INPUT: X : Training input data, T : Target, dims : (x, z, y) layer size, epochs : Number of epochs\n",
    "    #        lr : Learning rate, batchsize :  Size of mini-batch\n",
    "            \n",
    "    # OUTPUT: W : Trained weights,  losses :Loss at each epoch\n",
    " \n",
    "    \n",
    "    rg = np.random.default_rng()\n",
    "    W = init(dims)\n",
    "    losses = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        order = rg.permutation(m)\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for k in range(0, m, batchsize):\n",
    "            batch = order[k:k + batchsize]\n",
    "            X_batch = X[:, batch]\n",
    "            T_batch = T[:, batch]\n",
    "            W, loss = backward(X_batch, T_batch, W, n=batchsize, lr=lr)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Epoch {e+1}, loss {epoch_loss:.4f}\")\n",
    "    \n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441220a",
   "metadata": {},
   "source": [
    "# FFNN: Multilayer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40474874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rg = np.random.default_rng() # random munber generator \n",
    "\n",
    "def init(dims):\n",
    "    \"\"\"\n",
    "    Initialize weights for a multi-layer feedforward neural network.\n",
    "    \n",
    "    INPUT:\n",
    "        dims : Layer dimensions [input, hidden1, hidden2, ..., output]\n",
    "    \n",
    "    OUTPUT:\n",
    "        W : List of weight matrices for each layer.\n",
    "            Each W[l] has shape (dims[l]+1, dims[l+1]) and includes bias weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    W = []\n",
    "    for l in range(len(dims) - 1):\n",
    "        \n",
    "        # initialization for ReLU layers: variance = 2 / fan_in, makes sure variance is stable for relu \n",
    "        fan_in = dims[l] + 1\n",
    "        scale = np.sqrt(2 / fan_in)\n",
    "        W.append(scale * rg.standard_normal(size=(fan_in, dims[l+1])))\n",
    "        \n",
    "    return W\n",
    "\n",
    "\n",
    "def forward(X, W):\n",
    "    \"\"\"\n",
    "    Perform the forward pass through all layers.\n",
    "    \n",
    "    INPUT:\n",
    "        X : Input data (shape: features × samples)\n",
    "        W : Weight matrices for each layer\n",
    "    \n",
    "    OUTPUT:\n",
    "        y : Output of forward with softmax aplied\n",
    "        h : List containing activations (hidden layers and final output pre-softmax)\n",
    "    \"\"\"\n",
    "    \n",
    "    h = []  # store activations of each layer\n",
    "    a = X   # input layer\n",
    "    \n",
    "    # Forward through hidden layers\n",
    "    for l in range(len(W) - 1):\n",
    "        # Add bias term\n",
    "        a = np.vstack((a, np.ones((1, a.shape[1]))))\n",
    "        \n",
    "        # Linear transform\n",
    "        z = W[l].T @ a\n",
    "        \n",
    "        # ReLU activation\n",
    "        a = np.maximum(0, z)\n",
    "        h.append(a)\n",
    "    \n",
    "    # Output layer (no ReLU, only linear before softmax)\n",
    "    a = np.vstack((a, np.ones((1, a.shape[1]))))\n",
    "    y_hat = W[-1].T @ a\n",
    "    h.append(y_hat)\n",
    "    \n",
    "    # Last element in h is pre-softmax output\n",
    "    y_hat = h[-1]\n",
    "    \n",
    "    # Numerically stable softmax\n",
    "    exp_y = np.exp(y_hat - np.max(y_hat, axis=0))\n",
    "    y = exp_y / exp_y.sum(axis=0)\n",
    "    \n",
    "    return y, h\n",
    "\n",
    "\n",
    "def backward(X, T, W, lr):\n",
    "    \"\"\"\n",
    "    Perform one backward pass and update weights.\n",
    "    \n",
    "    INPUT:\n",
    "        X : Input data (features × samples)\n",
    "        T : Target labels (one-hot encoded)\n",
    "        W : Current weight matrices\n",
    "        lr : Learning rate\n",
    "    \n",
    "    OUTPUT:\n",
    "        W : Updated weight matrices\n",
    "        loss : Total loss for this batch\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X.shape[1]\n",
    "    y, h = forward(X, W)\n",
    "\n",
    "    # --- Loss ---\n",
    "    loss = -np.sum(T * np.log(y + 1e-15)) # cross-entropy loss for multi-class classification\n",
    "    \n",
    "    # --- Backpropagation --- (Like in example above just more layers)\n",
    "    delta = y - T  # output layer error (simple example)\n",
    "    \n",
    "    # --- First layer update ---\n",
    "    x_bias = np.vstack((X, np.ones((1, n))))\n",
    "    Q0 = x_bias @ delta.T\n",
    "    W[0] -= (lr / n) * Q0\n",
    "    \n",
    "    # --- Update all other layers\n",
    "    \n",
    "    for l in range(len(W) - 1, 0, -1):\n",
    "        # Get hidden activation for this layer\n",
    "        h_l = h[l-1] if l-1 >= 0 else X\n",
    "        \n",
    "        # Add bias term\n",
    "        h_l = np.vstack((h_l, np.ones((1, h_l.shape[1]))))\n",
    "        \n",
    "        # Compute gradient for this layer\n",
    "        Q = h_l @ delta.T\n",
    "        \n",
    "        # Update weights\n",
    "        W[l] -= (lr / n) * Q\n",
    "        \n",
    "        # Backpropagate error to previous layer\n",
    "        W_hat = W[l][:-1, :]  # remove bias weights\n",
    "        \n",
    "        if l > 0: # compute delta for next layer\n",
    "            a_prime = (h[l-1] > 0).astype(float)  # ReLU derivative\n",
    "            delta = a_prime * (W_hat @ delta)\n",
    "    \n",
    "    return W, loss\n",
    "\n",
    "\n",
    "def train(X, T, W, epochs, lr, batchsize=1):\n",
    "    \"\"\"\n",
    "    Train the multi-layer neural network using gradient descent.\n",
    "    \n",
    "    INPUT:\n",
    "        X : Input data (features × samples)\n",
    "        T : Target one-hot labels (classes × samples)\n",
    "        W : Initialized weight matrices\n",
    "        epochs :  Number of training epochs\n",
    "        lr : Learning rate\n",
    "        batchsize : Size of each training batch\n",
    "    \n",
    "    OUTPUT:\n",
    "        W : Trained weight matrices\n",
    "        losses : Total loss for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    losses = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        order = rg.permutation(m) # mini-batch is drawn in a random order\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for k in range(0, m, batchsize):\n",
    "            batch = order[k:k+batchsize]\n",
    "            X_batch = X[:, batch]\n",
    "            T_batch = T[:, batch]\n",
    "            \n",
    "            W, loss = backward(X_batch, T_batch, W, lr)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Epoch {e+1}, loss {epoch_loss:.4f}\")\n",
    "    \n",
    "    return W, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23528dcc",
   "metadata": {},
   "source": [
    "# Working w. MNIST minimal change \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4eb2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 37932.0995\n",
      "Epoch 2/100, Loss: 18044.1915\n",
      "Epoch 3/100, Loss: 15187.7958\n",
      "Epoch 4/100, Loss: 13638.4732\n",
      "Epoch 5/100, Loss: 12602.7522\n",
      "Epoch 6/100, Loss: 11802.6428\n",
      "Epoch 7/100, Loss: 11152.7841\n",
      "Epoch 8/100, Loss: 10658.2346\n",
      "Epoch 9/100, Loss: 10195.0382\n",
      "Epoch 10/100, Loss: 9813.7084\n",
      "Epoch 11/100, Loss: 9456.8015\n",
      "Epoch 12/100, Loss: 9180.1242\n",
      "Epoch 13/100, Loss: 8899.8566\n",
      "Epoch 14/100, Loss: 8653.8921\n",
      "Epoch 15/100, Loss: 8405.4606\n",
      "Epoch 16/100, Loss: 8185.0095\n",
      "Epoch 17/100, Loss: 7977.0305\n",
      "Epoch 18/100, Loss: 7784.6646\n",
      "Epoch 19/100, Loss: 7602.3839\n",
      "Epoch 20/100, Loss: 7453.3020\n",
      "Epoch 21/100, Loss: 7295.7094\n",
      "Epoch 22/100, Loss: 7148.7067\n",
      "Epoch 23/100, Loss: 6982.9027\n",
      "Epoch 24/100, Loss: 6854.6996\n",
      "Epoch 25/100, Loss: 6735.9956\n",
      "Epoch 26/100, Loss: 6601.0594\n",
      "Epoch 27/100, Loss: 6477.2278\n",
      "Epoch 28/100, Loss: 6373.1218\n",
      "Epoch 29/100, Loss: 6259.8047\n",
      "Epoch 30/100, Loss: 6130.8877\n",
      "Epoch 31/100, Loss: 6037.4217\n",
      "Epoch 32/100, Loss: 5947.5378\n",
      "Epoch 33/100, Loss: 5841.4754\n",
      "Epoch 34/100, Loss: 5762.2445\n",
      "Epoch 35/100, Loss: 5658.1835\n",
      "Epoch 36/100, Loss: 5571.3804\n",
      "Epoch 37/100, Loss: 5475.3138\n",
      "Epoch 38/100, Loss: 5405.1423\n",
      "Epoch 39/100, Loss: 5309.2368\n",
      "Epoch 40/100, Loss: 5243.9145\n",
      "Epoch 41/100, Loss: 5177.9081\n",
      "Epoch 42/100, Loss: 5100.1147\n",
      "Epoch 43/100, Loss: 5008.4209\n",
      "Epoch 44/100, Loss: 4930.4606\n",
      "Epoch 45/100, Loss: 4880.8568\n",
      "Epoch 46/100, Loss: 4806.1530\n",
      "Epoch 47/100, Loss: 4736.6913\n",
      "Epoch 48/100, Loss: 4668.2480\n",
      "Epoch 49/100, Loss: 4627.4669\n",
      "Epoch 50/100, Loss: 4549.8156\n",
      "Epoch 51/100, Loss: 4505.2306\n",
      "Epoch 52/100, Loss: 4454.0646\n",
      "Epoch 53/100, Loss: 4392.7695\n",
      "Epoch 54/100, Loss: 4342.5779\n",
      "Epoch 55/100, Loss: 4280.2936\n",
      "Epoch 56/100, Loss: 4222.0921\n",
      "Epoch 57/100, Loss: 4174.8025\n",
      "Epoch 58/100, Loss: 4128.7487\n",
      "Epoch 59/100, Loss: 4091.6974\n",
      "Epoch 60/100, Loss: 4029.5601\n",
      "Epoch 61/100, Loss: 3992.9428\n",
      "Epoch 62/100, Loss: 3927.7780\n",
      "Epoch 63/100, Loss: 3888.0827\n",
      "Epoch 64/100, Loss: 3859.6223\n",
      "Epoch 65/100, Loss: 3802.7906\n",
      "Epoch 66/100, Loss: 3765.6671\n",
      "Epoch 67/100, Loss: 3717.0296\n",
      "Epoch 68/100, Loss: 3682.7615\n",
      "Epoch 69/100, Loss: 3639.9438\n",
      "Epoch 70/100, Loss: 3586.0912\n",
      "Epoch 71/100, Loss: 3563.0847\n",
      "Epoch 72/100, Loss: 3526.9669\n",
      "Epoch 73/100, Loss: 3481.4427\n",
      "Epoch 74/100, Loss: 3458.6862\n",
      "Epoch 75/100, Loss: 3419.0198\n",
      "Epoch 76/100, Loss: 3397.2147\n",
      "Epoch 77/100, Loss: 3353.7058\n",
      "Epoch 78/100, Loss: 3320.8790\n",
      "Epoch 79/100, Loss: 3273.3814\n",
      "Epoch 80/100, Loss: 3244.7626\n",
      "Epoch 81/100, Loss: 3216.1873\n",
      "Epoch 82/100, Loss: 3166.4837\n",
      "Epoch 83/100, Loss: 3154.2515\n",
      "Epoch 84/100, Loss: 3124.3982\n",
      "Epoch 85/100, Loss: 3095.2950\n",
      "Epoch 86/100, Loss: 3056.0296\n",
      "Epoch 87/100, Loss: 3043.2624\n",
      "Epoch 88/100, Loss: 2996.4665\n",
      "Epoch 89/100, Loss: 2974.1055\n",
      "Epoch 90/100, Loss: 2944.2928\n",
      "Epoch 91/100, Loss: 2926.1748\n",
      "Epoch 92/100, Loss: 2892.0270\n",
      "Epoch 93/100, Loss: 2867.1118\n",
      "Epoch 94/100, Loss: 2827.6075\n",
      "Epoch 95/100, Loss: 2804.8103\n",
      "Epoch 96/100, Loss: 2779.3455\n",
      "Epoch 97/100, Loss: 2753.9872\n",
      "Epoch 98/100, Loss: 2749.0955\n",
      "Epoch 99/100, Loss: 2709.9116\n",
      "Epoch 100/100, Loss: 2679.7979\n",
      "Test Accuracy: 96.94%\n"
     ]
    }
   ],
   "source": [
    "### 1. Import Required Libraries\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow\n",
    "\n",
    "def softmax(y_hat):\n",
    "    y_hat = y_hat - np.max(y_hat, axis=0, keepdims=True)  # prevent overflow\n",
    "    exp_scores = np.exp(y_hat)\n",
    "    return exp_scores / np.sum(exp_scores, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "### 2. Load MNIST Data\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape and normalize inputs \n",
    "X_train = X_train.reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.reshape(-1, 28*28) / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "T_train = to_categorical(y_train, num_classes=10)\n",
    "T_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "\n",
    "### 3. Initialize Network Parameters\n",
    "\n",
    "def init(dims):\n",
    "    W = []\n",
    "    for i in range(len(dims) - 1):\n",
    "        W.append(np.random.randn(dims[i] + 1, dims[i + 1]) * np.sqrt(2 / (dims[i])))\n",
    "    return W\n",
    "\n",
    "dims = [784, 32, 32, 10]  # Input layer (784), hidden layer (128), output layer (10)\n",
    "W = init(dims)\n",
    "\n",
    "\n",
    "### 4. Define Forward Pass\n",
    "\n",
    "def forward(X, W):\n",
    "    h = []\n",
    "    a = X\n",
    "    for l in range(len(W) - 1):\n",
    "        a = np.vstack([a, np.ones(a.shape[1])])  # Add bias term\n",
    "        z = W[l].T @ a\n",
    "        a = np.maximum(0, z)  # ReLU activation\n",
    "        h.append(a)\n",
    "    a = np.vstack([a, np.ones(a.shape[1])])  # Add bias term\n",
    "    y_hat = W[-1].T @ a\n",
    "    y = softmax(y_hat)  # new stable version\n",
    "  # Softmax\n",
    "    return y, h\n",
    "\n",
    "\n",
    "### 5. Define Backward Pass\n",
    "\n",
    "def backward(X, T, W, h, eta):\n",
    "    m = X.shape[1]\n",
    "    y, _ = forward(X, W)\n",
    "    delta = y - T\n",
    "    for l in range(len(W) - 1, 0, -1):\n",
    "        a_prev = np.vstack([h[l-1], np.ones(h[l-1].shape[1])])  # Add bias term\n",
    "        Q = a_prev @ delta.T\n",
    "        W[l] -= (eta / m) * Q\n",
    "        delta = W[l][:-1, :] @ delta\n",
    "        delta *= h[l-1] > 0  # ReLU derivative\n",
    "    a_prev = np.vstack([X, np.ones(X.shape[1])])  # Add bias term\n",
    "    Q = a_prev @ delta.T\n",
    "    W[0] -= eta * Q\n",
    "    epsilon = 1e-12\n",
    "    loss = -np.sum(np.log(np.sum(y * T, axis=0) + epsilon))\n",
    "    return W, loss\n",
    "\n",
    "\n",
    "### 6. Training Loop\n",
    "\n",
    "def train(X, T, W, epochs, eta, batchsize=32):\n",
    "    m = X.shape[1]\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        order = np.random.permutation(m)\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, m, batchsize):\n",
    "            batch = order[i:i+batchsize]\n",
    "            X_batch = X[:, batch]\n",
    "            T_batch = T[:, batch]\n",
    "            _, h = forward(X_batch, W)\n",
    "            W, loss = backward(X_batch, T_batch, W, h, eta)\n",
    "            epoch_loss += loss\n",
    "        losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    return W, losses\n",
    "\n",
    "\n",
    "### 7. Train the Model\n",
    "\n",
    "epochs = 100\n",
    "eta = 0.001\n",
    "W, losses = train(X_train.T, T_train.T, W, epochs, eta)\n",
    "\n",
    "### 8. Evaluate the Model\n",
    "\n",
    "def predict(X, W):\n",
    "    y, _ = forward(X, W)\n",
    "    return np.argmax(y, axis=0)\n",
    "\n",
    "y_pred = predict(X_test.T, W)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
